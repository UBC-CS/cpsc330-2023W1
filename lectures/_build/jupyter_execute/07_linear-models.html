

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lecture 7: Linear Models &#8212; CPSC 330 Applied Machine Learning 2023W1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/_build/jupyter_execute/07_linear-models';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/UBC-CS-logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../_static/UBC-CS-logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/README.html">CPSC 330 Documents</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../01_intro.html">Lecture 1: Course Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../02_decision-trees.html">Lecture 2: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../03_ml-fundamentals.html">Lecture 3: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../04_kNNs-SVM-RBF.html">Lecture 4: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../05_preprocessing-pipelines.html">Lecture 5: Preprocessing and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../06_column-transformer-text-feats.html">Lecture 6: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and Text Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../07_linear-models.html">Lecture 7: Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../08_hyperparameter-optimization.html">Lecture 8: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../09_classification-metrics.html">Lecture 9: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../10_regression-metrics.html">Lecture 10: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../11_ensembles.html">Lecture 11: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../12_feat-importances.html">Lecture 12: Feature importances and model transparency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../13_feature-engineering-selection.html">Lecture 13: Feature engineering and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../14_K-Means.html">Lecture 14: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../15_DBSCAN-hierarchical.html">Lecture 15: More Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../16_recommender-systems.html">Lecture 16: Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../17_natural-language-processing.html">Lecture 17: Introduction to natural language processing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/03_class-demo.html">Lecture 3: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/04_class-demo.html">Lecture 4: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/05-06_class-demo.html">Lectures 5 and 6: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/07_class-demo.html">Lectures 7: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/09_class-demo.html">Exploring classification metrics</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../class_demos/14_class-demo.html">Lecture 14: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../class_demos/15_class-demo.html">Lecture 15: Class demo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Appendix-A.html">Appendix A</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Appendix-B.html">Appendix-B</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../attribution.html">Attributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/lectures/_build/jupyter_execute/07_linear-models.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 7: Linear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports-announcements-and-lo">Imports, Announcements, and LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements">Announcements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-video">Linear models [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-linear-regression">Prediction of linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-exactly-learning">What are we exactly learning?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-we-making-predictions">How are we making predictions?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-more-features">Generalizing to more features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-on-the-california-housing-dataset"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the California housing dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-alpha-of-ridge">Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients-and-intercept">Coefficients and intercept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-1">(iClicker) Exercise 7.1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients">Interpretation of coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-of-the-coefficients">Sign of the coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-of-the-coefficients">Magnitude of the coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-scaling">Importance of scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-video">Logistic regression [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-intuition">Logistic regression intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-for-the-motivating-example">Training data for the motivating example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-coefficients-associated-with-all-features">Learned coefficients associated with all features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-with-learned-weights">Predicting with learned weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-linear-classifier">Components of a linear classifier</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-on-the-cities-data">Logistic regression on the cities data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-learned-parameters">Accessing learned parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-learned-parameters">Prediction with learned parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#raw-scores">Raw scores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-of-logistic-regression">Decision boundary of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameter-of-logistic-regression">Main hyperparameter of logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-probability-scores-video">Predicting probability scores [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-proba"><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-calculate-these-probabilities">How does logistic regression calculate these probabilities?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function">The sigmoid function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-confident-cases">Least confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#most-confident-cases">Most confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#over-confident-cases">Over confident cases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-2">(iClicker) Exercise 7.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm">Linear SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-linear-models">Summary of linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">Main hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients-in-linear-models">Interpretation of coefficients in linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-of-linear-models">Strengths of linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-linear-models">Limitations of linear models</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><img alt="" src="lectures/_build/jupyter_execute/img/330-banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-7-linear-models">
<h1>Lecture 7: Linear Models<a class="headerlink" href="#lecture-7-linear-models" title="Permalink to this heading">#</a></h1>
<p>UBC 2023-24</p>
<p>Instructor: Varada Kolhatkar and Andrew Roth</p>
<section id="imports-announcements-and-lo">
<h2>Imports, Announcements, and LO<a class="headerlink" href="#imports-announcements-and-lo" title="Permalink to this heading">#</a></h2>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;code/.&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span>
<span class="kn">from</span> <span class="nn">plotting_functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.
Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">14</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span>
<span class="ne">---&gt; </span><span class="mi">14</span> <span class="kn">from</span> <span class="nn">plotting_functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;plotting_functions&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="announcements">
<h3>Announcements<a class="headerlink" href="#announcements" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Homework 3 is due on Oct 2nd.</p></li>
</ul>
</section>
</section>
<section id="learning-outcomes">
<h2>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Permalink to this heading">#</a></h2>
<p>From this lecture, students are expected to be able to:</p>
<ul class="simple">
<li><p>Explain the general intuition behind linear models;</p></li>
<li><p>Explain how <code class="docutils literal notranslate"><span class="pre">predict</span></code> works for linear regression;</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> model;</p></li>
<li><p>Demonstrate how the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> is related to the fundamental tradeoff;</p></li>
<li><p>Explain the difference between linear regression and logistic regression;</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model and <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> to get probability scores</p></li>
<li><p>Explain the advantages of getting probability scores instead of hard predictions during classification;</p></li>
<li><p>Broadly describe linear SVMs</p></li>
<li><p>Explain how can you interpret model predictions using coefficients learned by a linear model;</p></li>
<li><p>Explain the advantages and limitations of linear classifiers.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="linear-models-video">
<h2>Linear models [<a class="reference external" href="https://youtu.be/HXd1U2q4VFA">video</a>]<a class="headerlink" href="#linear-models-video" title="Permalink to this heading">#</a></h2>
<p><strong>Linear models</strong> is a fundamental and widely used class of models. They are called <strong>linear</strong> because they make a prediction using a <strong>linear function</strong> of the input features.</p>
<p>We will talk about three linear models:</p>
<ul class="simple">
<li><p>Linear regression</p></li>
<li><p>Logistic regression</p></li>
<li><p>Linear SVM (brief mention)</p></li>
</ul>
<section id="linear-regression">
<h3>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A very popular statistical model and has a long history.</p></li>
<li><p>Imagine a hypothetical regression problem of predicting weight of a snake given its length.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">X_1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">])</span>
<span class="n">snakes_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">snakes_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">77</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s2">&quot;length&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[[</span><span class="s2">&quot;length&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>73</th>
      <td>1.489130</td>
      <td>10.507995</td>
    </tr>
    <tr>
      <th>53</th>
      <td>1.073233</td>
      <td>7.658047</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1.622709</td>
      <td>9.748797</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.984653</td>
      <td>9.731572</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.484937</td>
      <td>3.016555</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s visualize the hypothetical snake data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight (target)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/5c8b3844ecc7196f24e43f985b78a3eb4c6ff21a40e7fd8ade1642bf71459424.png" src="../../../_images/5c8b3844ecc7196f24e43f985b78a3eb4c6ff21a40e7fd8ade1642bf71459424.png" />
</div>
</div>
<p>Let’s plot a linear regression model on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">r</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight (target)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/5274b879ea8b526e0f74cada88cb79b7b1e620d62eafeaf9ecd7d6e3fb157489.png" src="../../../_images/5274b879ea8b526e0f74cada88cb79b7b1e620d62eafeaf9ecd7d6e3fb157489.png" />
</div>
</div>
<p><strong>The orange line is the learned linear model.</strong></p>
</section>
<section id="prediction-of-linear-regression">
<h3>Prediction of linear regression<a class="headerlink" href="#prediction-of-linear-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given a snake length, we can use the model above to predict the target (i.e., the weight of the snake).</p></li>
<li><p>The prediction will be the corresponding weight on the orange line.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">snake_length</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">r</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">snake_length</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<section id="what-are-we-exactly-learning">
<h4>What are we exactly learning?<a class="headerlink" href="#what-are-we-exactly-learning" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The model above is a line, which can be represented with a slope (i.e., coefficient or weight) and an intercept.</p></li>
<li><p>For the above model, we can access the slope (i.e., coefficient or weight) and the intercept using <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>, respectively.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span>  <span class="c1"># r is our linear regression object</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5.26370005])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">intercept_</span>  <span class="c1"># r is our linear regression object</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.259057547817185
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="how-are-we-making-predictions">
<h3>How are we making predictions?<a class="headerlink" href="#how-are-we-making-predictions" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given a feature value <span class="math notranslate nohighlight">\(x_1\)</span> and learned coefficient <span class="math notranslate nohighlight">\(w_1\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span>, we can get the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> with the following formula:
$<span class="math notranslate nohighlight">\(\hat{y} = w_1x_1 + b\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">snake_length</span> <span class="o">*</span> <span class="n">r</span><span class="o">.</span><span class="n">coef_</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">snake_length</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<p>Great! Now we exactly know how the model is making the prediction.</p>
</section>
<section id="generalizing-to-more-features">
<h3>Generalizing to more features<a class="headerlink" href="#generalizing-to-more-features" title="Permalink to this heading">#</a></h3>
<p>For more features, the model is a higher dimensional hyperplane and the general prediction formula looks as follows:</p>
<p><span class="math notranslate nohighlight">\(\hat{y} =\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_1\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_1\)</span> </font> <span class="math notranslate nohighlight">\(+ \dots +\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_d\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_d\)</span></font> + <font  color="green"> <span class="math notranslate nohighlight">\(b\)</span></font></p>
<p>where,</p>
<ul class="simple">
<li><p><font  color="blue"> (<span class="math notranslate nohighlight">\(x_1, \dots, x_d\)</span>) are input features </font></p></li>
<li><p><font  color="red"> (<span class="math notranslate nohighlight">\(w_1, \dots, w_d\)</span>) are coefficients or weights </font> (learned from the data)</p></li>
<li><p><font  color="green"> <span class="math notranslate nohighlight">\(b\)</span> is the bias which can be used to offset your hyperplane </font> (learned from the data)</p></li>
</ul>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Suppose these are the coefficients learned by a linear regression model on a hypothetical housing price prediction dataset.</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head text-right"><p>Learned coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bedrooms</p></td>
<td class="text-right"><p>0.20</p></td>
</tr>
<tr class="row-odd"><td><p>Bathrooms</p></td>
<td class="text-right"><p>0.11</p></td>
</tr>
<tr class="row-even"><td><p>Square Footage</p></td>
<td class="text-right"><p>0.002</p></td>
</tr>
<tr class="row-odd"><td><p>Age</p></td>
<td class="text-right"><p>-0.02</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Now given a new example, the target will be predicted as follows:
| Bedrooms | Bathrooms | Square Footage | Age |
|——————–|———————|—————-|—–|
| 3                  | 2                   | 1875           | 66  |</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b\]</div>
<div class="math notranslate nohighlight">
\[\text{predicted price}=  0.20 \times 3 + 0.11 \times 2 + 0.002 \times 1875 + (-0.02) \times 66 + b\]</div>
<p>When we call <code class="docutils literal notranslate"><span class="pre">fit</span></code>, a coefficient or weight is learned for each feature which tells us the role of that feature in prediction. These coefficients are learned from the training data.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In linear models for regression, the model is a line for a single feature, a plane for two features, and a hyperplane for higher dimensions. We are not yet ready to discuss how does linear regression learn these coefficients and intercept.</p>
</div>
</section>
<section id="ridge">
<h3><code class="docutils literal notranslate"><span class="pre">Ridge</span></code><a class="headerlink" href="#ridge" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> has a model called <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> for linear regression.</p></li>
<li><p>But if we use this “vanilla” version of linear regression, it may result in large coefficients and unexpected results.</p></li>
<li><p>So instead of using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>, we will always use another linear model called <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, which is a linear regression model with a complexity hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>  <span class="c1"># DO NOT USE IT IN THIS COURSE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>  <span class="c1"># USE THIS INSTEAD</span>
</pre></div>
</div>
</div>
</div>
<section id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this heading">#</a></h4>
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s built in regression dataset, the Boston Housing dataset. The task associated with this dataset is to predict the median value of homes in several Boston neighborhoods in the 1970s, using information such as crime rate in the neighbourhood, average number of rooms, proximity to the Charles River, highway accessibility, and so on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>


<span class="n">california</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">california</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">california</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">california</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.0000</td>
      <td>52.0</td>
      <td>5.303030</td>
      <td>1.082251</td>
      <td>725.0</td>
      <td>3.138528</td>
      <td>38.03</td>
      <td>-121.88</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.0000</td>
      <td>52.0</td>
      <td>5.506410</td>
      <td>1.134615</td>
      <td>1026.0</td>
      <td>3.288462</td>
      <td>34.00</td>
      <td>-118.30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.0474</td>
      <td>30.0</td>
      <td>5.419355</td>
      <td>1.006452</td>
      <td>858.0</td>
      <td>2.767742</td>
      <td>37.31</td>
      <td>-121.94</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.2794</td>
      <td>7.0</td>
      <td>5.546473</td>
      <td>1.044166</td>
      <td>5146.0</td>
      <td>3.392221</td>
      <td>34.46</td>
      <td>-117.20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.5551</td>
      <td>35.0</td>
      <td>4.018487</td>
      <td>1.016807</td>
      <td>1886.0</td>
      <td>3.169748</td>
      <td>37.35</td>
      <td>-121.86</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16507</th>
      <td>3.0185</td>
      <td>17.0</td>
      <td>4.205479</td>
      <td>0.863014</td>
      <td>434.0</td>
      <td>1.981735</td>
      <td>34.61</td>
      <td>-120.16</td>
    </tr>
    <tr>
      <th>16508</th>
      <td>12.6320</td>
      <td>5.0</td>
      <td>7.462963</td>
      <td>0.888889</td>
      <td>208.0</td>
      <td>3.851852</td>
      <td>34.44</td>
      <td>-119.31</td>
    </tr>
    <tr>
      <th>16509</th>
      <td>3.9808</td>
      <td>20.0</td>
      <td>5.678689</td>
      <td>1.006557</td>
      <td>999.0</td>
      <td>3.275410</td>
      <td>38.28</td>
      <td>-121.20</td>
    </tr>
    <tr>
      <th>16510</th>
      <td>5.8195</td>
      <td>25.0</td>
      <td>6.585513</td>
      <td>0.961771</td>
      <td>1645.0</td>
      <td>3.309859</td>
      <td>33.71</td>
      <td>-117.97</td>
    </tr>
    <tr>
      <th>16511</th>
      <td>3.7315</td>
      <td>20.0</td>
      <td>7.368304</td>
      <td>1.738839</td>
      <td>1085.0</td>
      <td>2.421875</td>
      <td>37.58</td>
      <td>-118.74</td>
    </tr>
  </tbody>
</table>
<p>16512 rows × 8 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">california</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
</pre></div>
</div>
</div>
</div>
</section>
<section id="ridge-on-the-california-housing-dataset">
<h4><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the California housing dataset<a class="headerlink" href="#ridge-on-the-california-housing-dataset" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.004744</td>
      <td>0.000475</td>
      <td>0.599492</td>
      <td>0.608050</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.002746</td>
      <td>0.000391</td>
      <td>0.613507</td>
      <td>0.604619</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.003004</td>
      <td>0.000458</td>
      <td>0.598100</td>
      <td>0.608468</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.002839</td>
      <td>0.000398</td>
      <td>0.612621</td>
      <td>0.604797</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.002535</td>
      <td>0.000370</td>
      <td>0.601790</td>
      <td>0.606902</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hyperparameter-alpha-of-ridge">
<h4>Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code><a class="headerlink" href="#hyperparameter-alpha-of-ridge" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Ridge has hyperparameters just like the rest of the models we learned.</p></li>
<li><p>The alpha hyperparameter is what makes <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> different from vanilla <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p></li>
<li><p>Similar to the other hyperparameters that we saw, <code class="docutils literal notranslate"><span class="pre">alpha</span></code> controls the fundamental tradeoff.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we set alpha=0 that is the same as using LinearRegression.</p>
</div>
<p>Let’s examine the effect of <code class="docutils literal notranslate"><span class="pre">alpha</span></code> on the fundamental tradeoff.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;mean_train_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
    <span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">]:</span>
    <span class="n">pipe_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_ridge</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alpha</th>
      <th>mean_train_scores</th>
      <th>mean_cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.001</td>
      <td>0.606567</td>
      <td>0.605101</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.010</td>
      <td>0.606567</td>
      <td>0.605101</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.100</td>
      <td>0.606567</td>
      <td>0.605101</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.000</td>
      <td>0.606567</td>
      <td>0.605102</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10.000</td>
      <td>0.606557</td>
      <td>0.605100</td>
    </tr>
    <tr>
      <th>5</th>
      <td>100.000</td>
      <td>0.605696</td>
      <td>0.604320</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1000.000</td>
      <td>0.579432</td>
      <td>0.578523</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10000.000</td>
      <td>0.431137</td>
      <td>0.430868</td>
    </tr>
    <tr>
      <th>8</th>
      <td>100000.000</td>
      <td>0.116167</td>
      <td>0.115956</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Here we do not really see overfitting but in general,</p>
<ul class="simple">
<li><p>larger <code class="docutils literal notranslate"><span class="pre">alpha</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> likely to underfit</p></li>
<li><p>smaller <code class="docutils literal notranslate"><span class="pre">alpha</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> likely to overfit</p></li>
</ul>
</section>
<section id="coefficients-and-intercept">
<h4>Coefficients and intercept<a class="headerlink" href="#coefficients-and-intercept" title="Permalink to this heading">#</a></h4>
<p>The model learns</p>
<ul class="simple">
<li><p>coefficients associated with each feature</p></li>
<li><p>the intercept or bias</p></li>
</ul>
<p>Let’s examine the coefficients learned by the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
<span class="n">pipe_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">california</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficients&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MedInc</th>
      <td>0.827039</td>
    </tr>
    <tr>
      <th>HouseAge</th>
      <td>0.117058</td>
    </tr>
    <tr>
      <th>AveRooms</th>
      <td>-0.265273</td>
    </tr>
    <tr>
      <th>AveBedrms</th>
      <td>0.307521</td>
    </tr>
    <tr>
      <th>Population</th>
      <td>-0.003194</td>
    </tr>
    <tr>
      <th>AveOccup</th>
      <td>-0.039498</td>
    </tr>
    <tr>
      <th>Latitude</th>
      <td>-0.895963</td>
    </tr>
    <tr>
      <th>Longitude</th>
      <td>-0.866110</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The model also learns an intercept (bias).</p></li>
<li><p>For each prediction, we are adding this amount irrespective of the feature values.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.0667579112160865
</pre></div>
</div>
</div>
</div>
<p>Can we use this information to interpret model predictions?</p>
</section>
</section>
</section>
<section id="questions-for-you">
<h2>❓❓ Questions for you<a class="headerlink" href="#questions-for-you" title="Permalink to this heading">#</a></h2>
<section id="iclicker-exercise-7-1">
<h3>(iClicker) Exercise 7.1<a class="headerlink" href="#iclicker-exercise-7-1" title="Permalink to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/3DP5H</strong></p>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) Increasing the hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> is likely to decrease model complexity.</p></li>
<li><p>(B) <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> can be used with datasets that have multiple features.</p></li>
<li><p>(C) With Ridge, we learn one coefficient per training example.</p></li>
<li><p>(D) If you train a linear regression model on a 2-dimensional problem (2 features), the model will learn 3 parameters: one for each feature and one for the bias term.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="interpretation-of-coefficients">
<h2>Interpretation of coefficients<a class="headerlink" href="#interpretation-of-coefficients" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>One of the main advantages of linear models is that they are relatively easy to interpret.</p></li>
<li><p>We have one coefficient per feature which kind of describes the role of the feature in the prediction according to the model.</p></li>
</ul>
<p>There are two pieces of information in the coefficients based on</p>
<ul class="simple">
<li><p>Sign</p></li>
<li><p>Magnitude</p></li>
</ul>
<section id="sign-of-the-coefficients">
<h3>Sign of the coefficients<a class="headerlink" href="#sign-of-the-coefficients" title="Permalink to this heading">#</a></h3>
<p>In the example below, for instance:</p>
<ul class="simple">
<li><p>MedInc (median income) has a <strong>positive coefficient</strong></p>
<ul>
<li><p>the prediction will be proportional to the feature value; as MedInc gets <strong>bigger</strong>, the median house value gets <strong>bigger</strong></p></li>
</ul>
</li>
<li><p>AveRooms (Average number of rooms) has a <strong>negative coefficient</strong></p>
<ul>
<li><p>the prediction will be inversely proportional to the feature value; as AveRooms gets <strong>bigger</strong>, the median house value gets <strong>smaller</strong></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">california</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficients&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MedInc</th>
      <td>0.827039</td>
    </tr>
    <tr>
      <th>HouseAge</th>
      <td>0.117058</td>
    </tr>
    <tr>
      <th>AveRooms</th>
      <td>-0.265273</td>
    </tr>
    <tr>
      <th>AveBedrms</th>
      <td>0.307521</td>
    </tr>
    <tr>
      <th>Population</th>
      <td>-0.003194</td>
    </tr>
    <tr>
      <th>AveOccup</th>
      <td>-0.039498</td>
    </tr>
    <tr>
      <th>Latitude</th>
      <td>-0.895963</td>
    </tr>
    <tr>
      <th>Longitude</th>
      <td>-0.866110</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="magnitude-of-the-coefficients">
<h4>Magnitude of the coefficients<a class="headerlink" href="#magnitude-of-the-coefficients" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Bigger magnitude <span class="math notranslate nohighlight">\(\rightarrow\)</span> bigger impact on the prediction</p></li>
<li><p>In the example below, both MedInc and AveBedrms have a positive impact on the prediction but MedInc would have a bigger positive impact because it’s feature value is going to be multiplied by a number with a bigger magnitude.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;coefficient&quot;</span><span class="p">:</span> <span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;magnitude&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
<span class="p">}</span>
<span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">california</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span>
    <span class="s2">&quot;magnitude&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">coef_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coefficient</th>
      <th>magnitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Latitude</th>
      <td>-0.895963</td>
      <td>0.895963</td>
    </tr>
    <tr>
      <th>Longitude</th>
      <td>-0.866110</td>
      <td>0.866110</td>
    </tr>
    <tr>
      <th>MedInc</th>
      <td>0.827039</td>
      <td>0.827039</td>
    </tr>
    <tr>
      <th>AveBedrms</th>
      <td>0.307521</td>
      <td>0.307521</td>
    </tr>
    <tr>
      <th>AveRooms</th>
      <td>-0.265273</td>
      <td>0.265273</td>
    </tr>
    <tr>
      <th>HouseAge</th>
      <td>0.117058</td>
      <td>0.117058</td>
    </tr>
    <tr>
      <th>AveOccup</th>
      <td>-0.039498</td>
      <td>0.039498</td>
    </tr>
    <tr>
      <th>Population</th>
      <td>-0.003194</td>
      <td>0.003194</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="importance-of-scaling">
<h3>Importance of scaling<a class="headerlink" href="#importance-of-scaling" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>When you are interpreting the model coefficients, scaling is crucial.</p></li>
<li><p>If you do not scale the data, features with smaller magnitude are going to get coefficients with bigger magnitude whereas features with bigger scale are going to get coefficients with smaller magnitude.</p></li>
<li><p>That said, when you scale the data, feature values become hard to interpret for humans!</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Take these coefficients with a grain of salt. They might not always match your intuitions. Also, they do not tell us about how the world works. They only tell us about how the prediction of your model works.</p>
</div>
<p><br><br></p>
</section>
</section>
<section id="id1">
<h2>❓❓ Questions for you<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Discuss the importance of scaling when interpreting linear regression coefficients.</p></li>
<li><p>What might be the meaning of complex vs simpler model in case of linear regression?</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="logistic-regression-video">
<h2>Logistic regression [<a class="reference external" href="https://youtu.be/56L5z_t22qE">video</a>]<a class="headerlink" href="#logistic-regression-video" title="Permalink to this heading">#</a></h2>
<section id="logistic-regression-intuition">
<h3>Logistic regression intuition<a class="headerlink" href="#logistic-regression-intuition" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A linear model for <strong>classification</strong>.</p></li>
<li><p>Similar to linear regression, it learns weights associated with each feature and the bias.</p></li>
<li><p>It applies a <strong>threshold</strong> on the raw output to decide whether the class is positive or negative.</p></li>
<li><p>In this lecture we will focus on the following aspects of logistic regression.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></p></li>
<li><p>how to use learned coefficients to interpret the model</p></li>
</ul>
</li>
</ul>
</section>
<section id="motivating-example">
<h3>Motivating example<a class="headerlink" href="#motivating-example" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Consider the problem of predicting sentiment expressed in movie reviews.</p></li>
</ul>
<section id="training-data-for-the-motivating-example">
<h4>Training data for the motivating example<a class="headerlink" href="#training-data-for-the-motivating-example" title="Permalink to this heading">#</a></h4>
<blockquote> 
    <p>Review 1: This movie was <b>excellent</b>! The performances were oscar-worthy!  👍 </p> 
    <p>Review 2: What a <b>boring</b> movie! I almost fell asleep twice while watching it. 👎 </p> 
    <p>Review 3: I enjoyed the movie. <b>Excellent</b>! 👍 </p>             
</blockquote>  
<ul class="simple">
<li><p>Targets: positive 👍 and negative 👎</p></li>
<li><p>Features: words (e.g., <em>excellent</em>, <em>flawless</em>, <em>boring</em>)</p></li>
</ul>
</section>
<section id="learned-coefficients-associated-with-all-features">
<h4>Learned coefficients associated with all features<a class="headerlink" href="#learned-coefficients-associated-with-all-features" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Suppose our vocabulary contains only the following 7 words.</p></li>
<li><p>A linear classifier learns <strong>weights</strong> or <strong>coefficients</strong> associated with the features (words in this example).</p></li>
<li><p>Let’s ignore bias for a bit.</p></li>
</ul>
<p><img alt="" src="lectures/_build/jupyter_execute/img/words_coeff.png" /></p>
<!-- <center>
<img src='./img/words_coeff.png' width="250" height="300" />
</center>  
 --></section>
<section id="predicting-with-learned-weights">
<h4>Predicting with learned weights<a class="headerlink" href="#predicting-with-learned-weights" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Use these learned coefficients to make predictions. For example, consider the following review <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
</ul>
<blockquote> 
It got a bit <b>boring</b> at times but the direction was <b>excellent</b> and the acting was <b>flawless</b>.
</blockquote>
- Feature vector for $x_i$: [1, 0, 1, 1, 0, 0, 0]<p><img alt="" src="lectures/_build/jupyter_execute/img/words_coeff.png" /></p>
<!-- <center>
<img src='./img/words_coeff.png' width="250" height="300" />
</center>  
 -->
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(score(x_i) = \)</span> coefficient(<em>boring</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> + coefficient(<em>excellent</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> + coefficient(<em>flawless</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> = <span class="math notranslate nohighlight">\(-1.40 + 1.93 + 1.43 = 1.96\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(1.96 &gt; 0\)</span> so predict the review as positive 👍.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;boring=1&quot;</span><span class="p">,</span> <span class="s2">&quot;excellent=1&quot;</span><span class="p">,</span> <span class="s2">&quot;flawless=1&quot;</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.40</span><span class="p">,</span> <span class="mf">1.93</span><span class="p">,</span> <span class="mf">1.43</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">plot_logistic_regression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weighted sum of the input features = 1.960 y_hat = pos
</pre></div>
</div>
<img alt="../../../_images/b878a23f996d3fae70cbe370cb3e05f77386c82c6265a686b51e325329055741.svg" src="../../../_images/b878a23f996d3fae70cbe370cb3e05f77386c82c6265a686b51e325329055741.svg" /></div>
</div>
<ul class="simple">
<li><p>So the prediction is based on the weighted sum of the input features.</p></li>
<li><p>Some feature are pulling the prediction towards positive sentiment and some are pulling it towards negative sentiment.</p></li>
<li><p>If the coefficient of <em>boring</em> had a bigger magnitude or <em>excellent</em> and <em>flawless</em> had smaller magnitudes, we would have predicted “neg”.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w_0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;boring=1&quot;</span><span class="p">,</span> <span class="s2">&quot;excellent=1&quot;</span><span class="p">,</span> <span class="s2">&quot;flawless=1&quot;</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.40</span><span class="p">,</span> <span class="mf">1.93</span><span class="p">,</span> <span class="mf">1.43</span><span class="p">]</span>
    <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_0</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">plot_logistic_regression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interactive</span><span class="p">(</span>
    <span class="n">f</span><span class="p">,</span>
    <span class="n">w_0</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">1.40</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1f6032b0ce2b48d3aaee7a79250592ee", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>In our case, for values for the coefficient of <em>boring</em> &lt; -3.36, the prediction would be negative.</p>
<p>A linear model learns these coefficients or weights from the training data!</p>
<p>So a linear classifier is a linear function of the input <code class="docutils literal notranslate"><span class="pre">X</span></code>, followed by a threshold.</p>
<div class="amsmath math notranslate nohighlight" id="equation-9f224128-57d1-4ccf-a413-fab43cc0b02b">
<span class="eqno">()<a class="headerlink" href="#equation-9f224128-57d1-4ccf-a413-fab43cc0b02b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{split}
z =&amp; w_1x_1 + \dots + w_dx_d + b\\
=&amp; w^Tx + b
\end{split}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{y} = \begin{cases}
         1, &amp; \text{if } z \geq r\\
         -1, &amp; \text{if } z &lt; r
\end{cases}\end{split}\]</div>
</section>
<section id="components-of-a-linear-classifier">
<h4>Components of a linear classifier<a class="headerlink" href="#components-of-a-linear-classifier" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>input features (<span class="math notranslate nohighlight">\(x_1, \dots, x_d\)</span>)</p></li>
<li><p>coefficients (weights) (<span class="math notranslate nohighlight">\(w_1, \dots, w_d\)</span>)</p></li>
<li><p>bias (<span class="math notranslate nohighlight">\(b\)</span> or <span class="math notranslate nohighlight">\(w_0\)</span>) (can be used to offset your hyperplane)</p></li>
<li><p>threshold (<span class="math notranslate nohighlight">\(r\)</span>)</p></li>
</ol>
<p>In our example before, we assumed <span class="math notranslate nohighlight">\(r=0\)</span> and <span class="math notranslate nohighlight">\(b=0\)</span>.</p>
</section>
</section>
<section id="logistic-regression-on-the-cities-data">
<h3>Logistic regression on the cities data<a class="headerlink" href="#logistic-regression-on-the-cities-data" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">cols</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>160</th>
      <td>-76.4813</td>
      <td>44.2307</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>127</th>
      <td>-81.2496</td>
      <td>42.9837</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>169</th>
      <td>-66.0580</td>
      <td>45.2788</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>188</th>
      <td>-73.2533</td>
      <td>45.3057</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>187</th>
      <td>-67.9245</td>
      <td>47.1652</td>
      <td>Canada</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s first try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> on the cities data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000260</td>
      <td>0.000538</td>
      <td>0.588235</td>
      <td>0.601504</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000152</td>
      <td>0.000310</td>
      <td>0.588235</td>
      <td>0.601504</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000146</td>
      <td>0.000246</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000538</td>
      <td>0.000283</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000167</td>
      <td>0.000248</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s try <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.005981</td>
      <td>0.000350</td>
      <td>0.852941</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.002167</td>
      <td>0.000262</td>
      <td>0.823529</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.002229</td>
      <td>0.000332</td>
      <td>0.696970</td>
      <td>0.858209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.002215</td>
      <td>0.000299</td>
      <td>0.787879</td>
      <td>0.843284</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.001897</td>
      <td>0.000243</td>
      <td>0.939394</td>
      <td>0.805970</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Logistic regression seems to be doing better than dummy classifier. But note that there is a lot of variation in the scores.</p>
</section>
<section id="accessing-learned-parameters">
<h3>Accessing learned parameters<a class="headerlink" href="#accessing-learned-parameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Recall that logistic regression learns the weights <span class="math notranslate nohighlight">\(w\)</span> and bias or intercept <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>How to access these weights?</p>
<ul>
<li><p>Similar to <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, we can access the weights and intercept using <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> attribute of the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> object, respectively.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>  <span class="c1"># these are the learned weights</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>  <span class="c1"># this is the bias term</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;features&quot;</span><span class="p">:</span> <span class="n">cols</span><span class="p">,</span> <span class="s2">&quot;coefficients&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.04108149 -0.33683126]]
Model intercept: [10.8869838]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>longitude</td>
      <td>-0.041081</td>
    </tr>
    <tr>
      <th>1</th>
      <td>latitude</td>
      <td>-0.336831</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Both negative weights</p></li>
<li><p>The weight of latitude is larger in magnitude.</p></li>
<li><p>This makes sense because Canada as a country lies above the USA and so we expect latitude values to contribute more to a prediction than longitude.</p></li>
</ul>
</section>
<section id="prediction-with-learned-parameters">
<h3>Prediction with learned parameters<a class="headerlink" href="#prediction-with-learned-parameters" title="Permalink to this heading">#</a></h3>
<p>Let’s predict target of a test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-64.8001,  46.098 ])
</pre></div>
</div>
</div>
</div>
<section id="raw-scores">
<h4>Raw scores<a class="headerlink" href="#raw-scores" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Calculate the raw score as: <code class="docutils literal notranslate"><span class="pre">y_hat</span> <span class="pre">=</span> <span class="pre">np.dot(w,</span> <span class="pre">x)</span> <span class="pre">+</span> <span class="pre">b</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
        <span class="n">example</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="mi">2</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="o">+</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.97817876])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Apply the threshold to the raw score.</p></li>
<li><p>Since the prediction is &lt; 0, predict “negative”.</p></li>
<li><p>What is a “negative” class in our context?</p></li>
<li><p>With logistic regression, the model randomly assigns one of the classes as a positive class and the other as negative.</p>
<ul>
<li><p>Usually it would alphabetically order the target and pick the first one as negative and second one as the positive class.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">classes_</span></code> attribute tells us which class is considered negative and which one is considered positive. - In this case, Canada is the negative class and USA is a positive class.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>So based on the negative score above (-1.978), we would predict Canada.</p></li>
<li><p>Let’s check the prediction given by the model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>Great! The predictions match! We exactly know how the model is making predictions.</p>
</section>
</section>
<section id="decision-boundary-of-logistic-regression">
<h3>Decision boundary of logistic regression<a class="headerlink" href="#decision-boundary-of-logistic-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The decision boundary of logistic regression is a <strong>hyperplane</strong> dividing the feature space in half.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/bb7e9204fdd7aeb20fd9190bbd583272422160f1ca135389fd538546c55f74a1.png" src="../../../_images/bb7e9204fdd7aeb20fd9190bbd583272422160f1ca135389fd538546c55f74a1.png" />
</div>
</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(d=2\)</span>, the decision boundary is a line (1-dimensional)</p></li>
<li><p>For <span class="math notranslate nohighlight">\(d=3\)</span>, the decision boundary is a plane (2-dimensional)</p></li>
<li><p>For <span class="math notranslate nohighlight">\(d\gt 3\)</span>, the decision boundary is a <span class="math notranslate nohighlight">\(d-1\)</span>-dimensional hyperplane</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="p">[</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">()],</span> <span class="n">axes</span>
<span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
        <span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span>
    <span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/44b4a44d4be7dafd1758077ab904c286247edfff38b09757aed1bb655581027e.png" src="../../../_images/44b4a44d4be7dafd1758077ab904c286247edfff38b09757aed1bb655581027e.png" />
</div>
</div>
<ul class="simple">
<li><p>Notice a linear decision boundary (a line in our case).</p></li>
<li><p>Compare it with  KNN or SVM RBF decision boundaries.</p></li>
</ul>
</section>
<section id="main-hyperparameter-of-logistic-regression">
<h3>Main hyperparameter of logistic regression<a class="headerlink" href="#main-hyperparameter-of-logistic-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">C</span></code> is the main hyperparameter which controls the fundamental trade-off.</p></li>
<li><p>We won’t really talk about the interpretation of this hyperparameter right now.</p></li>
<li><p>At a high level, the interpretation is similar to <code class="docutils literal notranslate"><span class="pre">C</span></code> of SVM RBF</p>
<ul>
<li><p>smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> might lead to underfitting</p></li>
<li><p>bigger <code class="docutils literal notranslate"><span class="pre">C</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> might lead to overfitting</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;mean_train_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
    <span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;C&quot;</span><span class="p">]:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
<span class="n">results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>mean_train_scores</th>
      <th>mean_cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0001</td>
      <td>0.664707</td>
      <td>0.658645</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0010</td>
      <td>0.784424</td>
      <td>0.790731</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0100</td>
      <td>0.827842</td>
      <td>0.826203</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>6</th>
      <td>100.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>9</th>
      <td>100000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="predicting-probability-scores-video">
<h2>Predicting probability scores [<a class="reference external" href="https://youtu.be/_OAK5KiGLg0">video</a>]<a class="headerlink" href="#predicting-probability-scores-video" title="Permalink to this heading">#</a></h2>
<section id="predict-proba">
<h3><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code><a class="headerlink" href="#predict-proba" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>So far in the context of classification problems, we focused on getting “hard” predictions.</p></li>
<li><p>Very often it’s useful to know “soft” predictions, i.e., how confident the model is with a given prediction.</p></li>
<li><p>For most of the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> classification models we can access this confidence score or probability score using a method called <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>.</p></li>
</ul>
<p>Let’s look at probability scores of logistic regression model for our test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-64.8001,  46.098 ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>  <span class="c1"># hard prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>  <span class="c1"># soft prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87848688, 0.12151312]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The output of <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> is the probability of each class.</p></li>
<li><p>In binary classification, we get probabilities associated with both classes (even though this information is redundant).</p></li>
<li><p>The first entry is the estimated probability of the first class and the second entry is the estimated probability of the second class from <code class="docutils literal notranslate"><span class="pre">model.classes_</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Because it’s a probability, the sum of the entries for both classes should always sum to 1.</p></li>
<li><p>Since the probabilities for the two classes sum to 1, exactly one of the classes will have a score &gt;=0.5, which is going to be our predicted class.</p></li>
</ul>
<section id="how-does-logistic-regression-calculate-these-probabilities">
<h4>How does logistic regression calculate these probabilities?<a class="headerlink" href="#how-does-logistic-regression-calculate-these-probabilities" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The weighted sum <span class="math notranslate nohighlight">\(w_1x_1 + \dots + w_dx_d + b\)</span> gives us “raw model output”.</p></li>
<li><p>For linear regression this would have been the prediction.</p></li>
<li><p>For logistic regression, you check the <strong>sign</strong> of this value.</p>
<ul>
<li><p>If positive (or 0), predict <span class="math notranslate nohighlight">\(+1\)</span>; if negative, predict <span class="math notranslate nohighlight">\(-1\)</span>.</p></li>
<li><p>These are “hard predictions”.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>You can also have “soft predictions”, aka <strong>predicted probabilities</strong>.</p>
<ul>
<li><p>To convert the raw model output into probabilities, instead of taking the sign, we apply the <strong>sigmoid</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="the-sigmoid-function">
<h4>The sigmoid function<a class="headerlink" href="#the-sigmoid-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The sigmoid function “squashes” the raw model output from any number to the range <span class="math notranslate nohighlight">\([0,1]\)</span> using the following formula, where <span class="math notranslate nohighlight">\(x\)</span> is the raw model output.
$<span class="math notranslate nohighlight">\(\frac{1}{1+e^{-x}}\)</span>$</p></li>
<li><p>Then we can interpret the output as probabilities.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">raw_model_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s2">&quot;--k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s2">&quot;--k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;raw model output, $w^Tx$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;the sigmoid function&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/6780219cd3d32f91773f0767bb4edd67c78e6208159a30323b1b27009307912f.png" src="../../../_images/6780219cd3d32f91773f0767bb4edd67c78e6208159a30323b1b27009307912f.png" />
</div>
</div>
<ul class="simple">
<li><p>Recall our hard predictions that check the sign of <span class="math notranslate nohighlight">\(w^Tx\)</span>, or, in other words, whether or not it is <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<ul>
<li><p>The threshold <span class="math notranslate nohighlight">\(w^Tx=0\)</span> corresponds to <span class="math notranslate nohighlight">\(p=0.5\)</span>.</p></li>
<li><p>In other words, if our predicted probability is <span class="math notranslate nohighlight">\(\geq 0.5\)</span> then our hard prediction is <span class="math notranslate nohighlight">\(+1\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Let’s get the probability score by calling sigmoid on the raw model output for our test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
        <span class="n">example</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="mi">2</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="o">+</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.12151312])
</pre></div>
</div>
</div>
</div>
<p>This is the probability score of the positive class, which is USA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87848688, 0.12151312]])
</pre></div>
</div>
</div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, we get the same probability score for USA!!</p>
<ul class="simple">
<li><p>Let’s visualize probability scores for some examples.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">],</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probabilities&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probabilities</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7046068097086481, 0.2953931902913519]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.563016906204013, 0.436983093795987]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.8389680973255864, 0.16103190267441364]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7964150775404333, 0.20358492245956678]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.9010806652340972, 0.0989193347659027]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7753006388010791, 0.2246993611989209]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.030740704606528002, 0.969259295393472]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.6880304799160921, 0.3119695200839079]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7891358587234142, 0.21086414127658581]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.006546969753885579, 0.9934530302461144]</td>
    </tr>
    <tr>
      <th>10</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.27874195848431016, 0.7212580415156898]</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.838887714664494, 0.16111228533550606]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The actual <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">y_hat</span></code> match in most of the cases but in some cases the model is more confident about the prediction than others.</p>
</section>
<section id="least-confident-cases">
<h4>Least confident cases<a class="headerlink" href="#least-confident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is least confident about the prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">least_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">141</span><span class="p">]]</span>
<span class="n">least_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ -79.7599,   43.6858],
       [-123.078 ,   48.9854]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">least_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">141</span><span class="p">]]</span>
<span class="n">least_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">least_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">least_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">least_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.634392</td>
      <td>0.365608</td>
    </tr>
    <tr>
      <th>1</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.635666</td>
      <td>0.364334</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">least_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">least_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">least_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/84a8681c99d84117fe9aa1c72c1da3fbbeceb4a7df986cffdbec2cb241049f54.png" src="../../../_images/84a8681c99d84117fe9aa1c72c1da3fbbeceb4a7df986cffdbec2cb241049f54.png" />
</div>
</div>
<p>The points are close to the decision boundary which makes sense.</p>
</section>
<section id="most-confident-cases">
<h4>Most confident cases<a class="headerlink" href="#most-confident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is most confident about the prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">37</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">most_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-110.9748,   32.2229],
       [ -67.9245,   47.1652]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">37</span><span class="p">,</span> <span class="mi">165</span><span class="p">]]</span>
<span class="n">most_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;USA&#39;, &#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">most_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">most_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">most_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.010028</td>
      <td>0.989972</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.901081</td>
      <td>0.098919</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-110.9748,   32.2229],
       [ -67.9245,   47.1652]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">most_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">most_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">most_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/913da3d07ff7d31114e9b5dcb500ecbc487f266284b63e1ab48b9c832547c517.png" src="../../../_images/913da3d07ff7d31114e9b5dcb500ecbc487f266284b63e1ab48b9c832547c517.png" />
</div>
</div>
<p>The points are far away from the decision boundary which makes sense.</p>
</section>
<section id="over-confident-cases">
<h4>Over confident cases<a class="headerlink" href="#over-confident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is confident about the prediction but the prediction is wrong.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">55</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([25, 55, 98]),)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">over_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">98</span><span class="p">,</span> <span class="mi">25</span><span class="p">]]</span>
<span class="n">over_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-134.4197,   58.3019],
       [-129.9912,   55.9383]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">over_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">over_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">over_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">over_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">over_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.961902</td>
      <td>0.038098</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.931792</td>
      <td>0.068208</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">over_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">over_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">over_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/727b35f1c0607f6bf5ec2f5a0abd3a91af54bf3a9c916a1e8ce84b6b6a69c2c7.png" src="../../../_images/727b35f1c0607f6bf5ec2f5a0abd3a91af54bf3a9c916a1e8ce84b6b6a69c2c7.png" />
</div>
</div>
<ul class="simple">
<li><p>The cities are far away from the decision boundary. So the model is pretty confident about the prediction.</p></li>
<li><p>But the cities are likely to be from Alaska and our linear model is not able to capture that this part belong to the USA and not Canada.</p></li>
</ul>
<p>Below we are using colour to represent prediction probabilities. If you are closer to the border, the model is less confident whereas the model is more confident about the mainland cities, which makes sense.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Train class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;Train class 1&quot;</span><span class="p">],</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">))</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">scores_image</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">plot_2d_scores</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cm</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span>
<span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">scores_image</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/5b5ec8f4632b2885eb00704fa51506da91728a31b32d3edec24c38c07e95865c.png" src="../../../_images/5b5ec8f4632b2885eb00704fa51506da91728a31b32d3edec24c38c07e95865c.png" />
</div>
</div>
<p>Sometimes a complex model that is overfitted, tends to make more confident predictions, even if they are wrong, whereas a simpler model tends to make predictions with more uncertainty.</p>
<p>To summarize,</p>
<ul class="simple">
<li><p>With hard predictions, we only know the class.</p></li>
<li><p>With probability scores we know how confident the model is with certain predictions, which can be useful in understanding the model better.</p></li>
</ul>
</section>
</section>
</section>
<section id="id2">
<h2>❓❓ Questions for you<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<section id="iclicker-exercise-7-2">
<h3>(iClicker) Exercise 7.2<a class="headerlink" href="#iclicker-exercise-7-2" title="Permalink to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/3DP5H</strong></p>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) Increasing logistic regression’s <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter increases model complexity.</p></li>
<li><p>(B) The raw output score can be used to calculate the probability score for a given prediction.</p></li>
<li><p>(C) For linear classifier trained on <span class="math notranslate nohighlight">\(d\)</span> features, the decision boundary is a <span class="math notranslate nohighlight">\(d-1\)</span>-dimensional hyperparlane.</p></li>
<li><p>(D) A linear model is likely to be uncertain about the data points close to the decision boundary.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="linear-svm">
<h3>Linear SVM<a class="headerlink" href="#linear-svm" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We have seen non-linear SVM with RBF kernel before. This is the default SVC model in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> because it tends to work better in many cases.</p></li>
<li><p>There is also a linear SVM. You can pass <code class="docutils literal notranslate"><span class="pre">kernel=&quot;linear&quot;</span></code> to create a linear SVM.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="k">for</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SVM RBF&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Linear SVM&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/f9668c521d04d14a60c2ca7f2518086b49b64453db9e8b6d44fcb13c0b1e3abc.png" src="../../../_images/f9668c521d04d14a60c2ca7f2518086b49b64453db9e8b6d44fcb13c0b1e3abc.png" />
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code> method of linear SVM and logistic regression works the same way.</p></li>
<li><p>We can get <code class="docutils literal notranslate"><span class="pre">coef_</span></code> associated with the features and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> using a Linear SVM model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="n">linear_svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">linear_svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">linear_svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.0195598  -0.23640124]]
Model intercept: [8.22811601]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.04108149 -0.33683126]]
Model intercept: [10.8869838]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Note that the coefficients and intercept are slightly different for logistic regression.</p></li>
<li><p>This is because the <code class="docutils literal notranslate"><span class="pre">fit</span></code> for linear SVM and logistic regression are different.</p></li>
</ul>
<p><br><br><br><br></p>
<p><br><br><br><br></p>
</section>
</section>
<section id="summary-of-linear-models">
<h2>Summary of linear models<a class="headerlink" href="#summary-of-linear-models" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Linear regression is a linear model for regression whereas logistic regression is a linear model for classification.</p></li>
<li><p>Both these models learn one coefficient per feature, plus an intercept.</p></li>
</ul>
<section id="main-hyperparameters">
<h3>Main hyperparameters<a class="headerlink" href="#main-hyperparameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The main hyperparameter is the “regularization” hyperparameter controlling the fundamental tradeoff.</p>
<ul>
<li><p>Logistic Regression: <code class="docutils literal notranslate"><span class="pre">C</span></code></p></li>
<li><p>Linear SVM: <code class="docutils literal notranslate"><span class="pre">C</span></code></p></li>
<li><p>Ridge: <code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="interpretation-of-coefficients-in-linear-models">
<h3>Interpretation of coefficients in linear models<a class="headerlink" href="#interpretation-of-coefficients-in-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(j\)</span>th coefficient tells us how feature <span class="math notranslate nohighlight">\(j\)</span> affects the prediction</p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j &gt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward predicting <span class="math notranslate nohighlight">\(+1\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j &lt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward prediction <span class="math notranslate nohighlight">\(-1\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j == 0\)</span> then the feature is not used in making a prediction</p></li>
</ul>
</section>
<section id="strengths-of-linear-models">
<h3>Strengths of linear models<a class="headerlink" href="#strengths-of-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Fast to train and predict</p></li>
<li><p>Scale to large datasets and work well with sparse data</p></li>
<li><p>Relatively easy to understand and interpret the predictions</p></li>
<li><p>Perform well when there is a large number of features</p></li>
</ul>
</section>
<section id="limitations-of-linear-models">
<h3>Limitations of linear models<a class="headerlink" href="#limitations-of-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Is your data “linearly separable”? Can you draw a hyperplane between these datapoints that separates them with 0 error.</p>
<ul>
<li><p>If the training examples can be separated by a linear decision rule, they are <strong>linearly separable</strong>.</p></li>
</ul>
</li>
</ul>
<p>A few questions you might be thinking about</p>
<ul class="simple">
<li><p>How often the real-life data is linearly separable?</p></li>
<li><p>Is the following XOR function linearly separable?</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>$<span class="math notranslate nohighlight">\(x_1\)</span>$</p></th>
<th class="head"><p>$<span class="math notranslate nohighlight">\(x_2\)</span>$</p></th>
<th class="head"><p>target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Are linear classifiers very limiting because of this?</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "cpsc330"
        },
        kernelOptions: {
            name: "cpsc330",
            path: "./lectures/_build/jupyter_execute"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'cpsc330'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports-announcements-and-lo">Imports, Announcements, and LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements">Announcements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-video">Linear models [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-linear-regression">Prediction of linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-exactly-learning">What are we exactly learning?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-we-making-predictions">How are we making predictions?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-more-features">Generalizing to more features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-on-the-california-housing-dataset"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the California housing dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-alpha-of-ridge">Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients-and-intercept">Coefficients and intercept</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-1">(iClicker) Exercise 7.1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients">Interpretation of coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-of-the-coefficients">Sign of the coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-of-the-coefficients">Magnitude of the coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-scaling">Importance of scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-video">Logistic regression [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-intuition">Logistic regression intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-for-the-motivating-example">Training data for the motivating example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-coefficients-associated-with-all-features">Learned coefficients associated with all features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-with-learned-weights">Predicting with learned weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-linear-classifier">Components of a linear classifier</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-on-the-cities-data">Logistic regression on the cities data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-learned-parameters">Accessing learned parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-learned-parameters">Prediction with learned parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#raw-scores">Raw scores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-of-logistic-regression">Decision boundary of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameter-of-logistic-regression">Main hyperparameter of logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-probability-scores-video">Predicting probability scores [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-proba"><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-calculate-these-probabilities">How does logistic regression calculate these probabilities?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function">The sigmoid function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-confident-cases">Least confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#most-confident-cases">Most confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#over-confident-cases">Over confident cases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-2">(iClicker) Exercise 7.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm">Linear SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-linear-models">Summary of linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">Main hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients-in-linear-models">Interpretation of coefficients in linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-of-linear-models">Strengths of linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-linear-models">Limitations of linear models</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>