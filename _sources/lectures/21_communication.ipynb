{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 21: Communication\n",
    "\n",
    "UBC 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar and Andrew Roth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os \n",
    "sys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\n",
    "from plotting_functions import *\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "- When communicating about applied ML, tailor an explanation to the intended audience.\n",
    "- Apply best practices of technical communication, such as bottom-up explanations and reader-centric writing.\n",
    "- Given an ML problem, analyze the decision being made and the objectives.\n",
    "- Avoid the pitfall of thinking about ML as coding in isolation; build the habit of relating your work to the surrounding context and stakeholders.\n",
    "- Interpret a confidence score or credence, e.g. what does it mean to be 5% confident that a statement is true.\n",
    "- Maintain a healthy skepticism of `predict_proba` scores and their possible interpretation as credences.\n",
    "- Be careful and precise when communicating confidence to stakeholders in an ML project.\n",
    "- Identify misleading visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcements\n",
    "\n",
    "- hw9 (last one) is released (due on December 7th at 11:59pm)\n",
    "- Solutions to hw7 and hw8 are available in Canvas under \"Files\".\n",
    "- Next lecture is a guest lecture by [Giulia Toti](https://www.gtoti.com/). \n",
    "- Final exam\n",
    "    - CPSC330 101 Sun Dec 17 2023 | 15:30 pm ESB 1013\n",
    "    - **CPSC330 102 Sun Dec 17 2023 | 15:30 pm CIRS 1250**    \n",
    "    - Format will be similar to the midterm. \n",
    "    - We'll release some practice questions later next week. \n",
    "- Almost there, hang in there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you\n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/SNBF**\n",
    "\n",
    "For our last class on December 7th, we have a couple of options, and we'd like to know your preference. Please choose one of the following options:\n",
    "\n",
    "- (A) **Introduction to Large Language Models (LLMs)**: An **optional** brief session where we explore what Large Language Models are, how they work, and a bit about the [Hugging Face](https://huggingface.co/docs/transformers/index) library. We won't have any questions on this material in the final exam.  \n",
    "- (B) **Final exam review session**: A review session where we go over key topics and answer any questions you might have for the upcoming final exam.\n",
    "- (C) **Combined session**: The first half of the class will be an optional brief session on LLMs, followed by a final exam review in the second half.  \n",
    "- (D) **Relax and take a day off!** A well-deserved break where we can relax and recharge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## (iClicker) Exercise 21.2\n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/SNBF**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) Right censoring occurs when the endpoint of event has not been observed for all study subjects by the end of the study period.\n",
    "- (B) Right censoring implies that the data is missing completely at random.\n",
    "- (C) In the presence of right-censored data, binary classification models can be applied directly without any modifications or special considerations.\n",
    "- (D) If we apply the `Ridge` regression model to predict tenure in right censored data, we are likely to underestimate it because the tenure observed in our data is shorter than what it would be in reality.\n",
    "- (E) In survival analysis, unlike typical supervised machine learning problems, it is beneficial to make predictions on training examples where customers have not yet churned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- What is right-censored data?\n",
    "- What happens when we treat right-censored data the same as \"regular\" data?\n",
    "    - Predicting churn vs. no churn\n",
    "    - Predicting tenure\n",
    "        - Throw away people who haven't churned\n",
    "        - Assume everyone churns today\n",
    "- Survival analysis encompasses predicting both churn and tenure and deals with censoring and can make rich and interesting predictions!\n",
    "    - We can get survival curves which show the probability of survival over time.\n",
    "    - KM model $\\rightarrow$ doesn't look at features\n",
    "    - CPH model $\\rightarrow$ like linear regression, does look at the features and provides coefficients associated with each feature.\n",
    "    \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Attribution\n",
    "\n",
    "- The first part of this lecture is adapted from [DSCI 542](https://github.com/UBC-MDS/DSCI_542_comm-arg), created by [David Laing](https://davidklaing.com/).\n",
    "- The visualization component of this lecture benefited from discussions with  [DSCI 531](https://github.com/UBC-MDS/DSCI_531_viz-1) instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why should we care about effective communication?\n",
    "\n",
    "- Most ML practitioners work in an organization with >1 people.\n",
    "- There will very likely be stakeholders other than yourself.\n",
    "- Some of them might not have any background in ML or computer science. \n",
    "- If your ML model is going to automate some important decisions in the organization you need to be able to explain \n",
    "    - What does a particular result mean? \n",
    "    - When does the model work?\n",
    "    - What are the risks? When does it fail?\n",
    "    - Why the model made a certain prediction for a particular example?  \n",
    "    - What are the consequences of using your model?\n",
    "- If you are able to convince your manager that using is model is beneficial, then only there are chances of your work going in production. \n",
    "- That said, you want to be honest when discussing the aspects above. If you mis-communicate the performance of your model, people will find out when the deployed model does not quite give similar performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the problems with the following headline? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/ml-communication.png)\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does \"accuracy\" here refer to the accuracy in ML?\n",
    "- Is there class imbalance?\n",
    "- Are they reporting numbers on the training data or validation or test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main issues in ML-related communication \n",
    "\n",
    "- Overstating one's results / unable to articulate the limitations\n",
    "- Unable to explain the predictions\n",
    "- Can we trust test error?\n",
    "- Why did a particular model (e.g., CatBoost) make that prediction?\n",
    "- What does it mean if `predict_proba` outputs 0.9?\n",
    "\n",
    "These issues are there because these things are actually very hard to explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity: explaining `GridSearchCV` (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Below are two possible explanations of `GridSearchCV` pitched to different audiences. Read them both and then follow the instructions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explanation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms, like an airplane's cockpit, typically involve a bunch of knobs and switches that need to be set.\n",
    "\n",
    "![](https://i.pinimg.com/236x/ea/43/f3/ea43f3c7f3a8c92d884ce012c77628fd--cockpit-gauges.jpg)\n",
    "\n",
    "For example, check out the documentation of the popular random forest algorithm [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Here's a list of the function arguments, along with their default values (from the documentation):\n",
    "\n",
    "> class sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "Holy cow, that's a lot of knobs and switches! As a machine learning practitioner, how am I supposed to choose `n_estimators`? Should I leave it at the default of 100? Or try 1000? What about `criterion` or `class_weight` for that matter? Should I trust the defaults?\n",
    "\n",
    "Enter [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to save the day. The general strategy here is to choose the settings that perform best on the specific task of interest. So I can't say `n_estimators=100` is better than `n_estimators=1000` without knowing what problem I'm working on. For a specific problem, you usually have a numerical score that measures performance. `GridSearchCV` is part of the popular [scikit-learn](https://scikit-learn.org/) Python machine learning library. It works by searching over various settings and tells you which one worked best on your problem. \n",
    "\n",
    "The \"grid\" in \"grid search\" comes from the fact that tries all possible combinations on a grid. For example, if you want it to consider setting `n_estimators` to 100, 150 or 200, and you want it to consider setting `criterion` to `'gini'` or `'entropy'`, then it will search over all 6 possible combinations in a grid of 3 possible values by 2 possible values: \n",
    "\n",
    "|                    | `criterion='gini'` | `criterion='entropy'` |\n",
    "|----------------------|--------|---------|\n",
    "| `n_estimators=100` |    1     |    2     |\n",
    "| `n_estimators=150` |    3     |    4     |\n",
    "| `n_estimators=200` |    5     |    6     |\n",
    "\n",
    "Here is a code sample that uses `GridSearchCV` to select from the 6 options we just mentioned. The problem being solved is classifying images of handwritten digits into the 10 digit categories (0-9). I chose this because the dataset is conveniently built in to scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load a dataset\n",
    "data = datasets.load_digits()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# set up the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=123),\n",
    "    param_grid={\"n_estimators\": [100, 150, 200], \"criterion\": [\"gini\", \"entropy\"]},\n",
    ")\n",
    "\n",
    "# run the grid search\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, the grid search selected `criterion='gini', n_estimators=100`, which was one of our 6 options above (specifically Option 1).\n",
    "\n",
    "By the way, these \"knobs\" we've been setting are called [_hyperparameters_](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning) and the process of setting these hyperparameters automatically is called [_hyperparameter optimization_](https://en.wikipedia.org/wiki/Hyperparameter_optimization) or _hyperparameter tuning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~400 words, not including code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explanation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998\n",
    "\n",
    "~400 words, not including code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you like about each explanation?\n",
    "- What do you dislike about each explanation?\n",
    "- What do you think is the intended audience for each explanation?\n",
    "- Which explanation do you think is more effective overall for someone on Day 1 of CPSC 330?\n",
    "- Each explanation has an image. Which one is more effective? What are the pros/cons?\n",
    "- Each explanation has some sample code. Which one is more effective? What are the pros/cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done reading, take ~5 min to consider the discussion questions above. Paste your answer to **at least one** of the above questions in [this Google document](https://docs.google.com/document/d/1Nqpcx4yHuVhyrDFrpxbBnduoJC4VFnvVybqzIA1aUmM/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principles of good explanations (~15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Concepts *then* labels, not the other way around\n",
    "\n",
    "The first explanation start with an analogy for the concept (and the label is left until the very end):\n",
    "\n",
    "> Machine learning algorithms, like an airplane's cockpit, typically involve a bunch of knobs and switches that need to be set.\n",
    "\n",
    "In the second explanation, the first sentence is wasted on anyone who doesn't already know what \"hyperparameter tuning\" means:\n",
    "\n",
    "> Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. \n",
    "\n",
    "The effectiveness of these different statements depend on your audience. \n",
    "\n",
    "See [this video](https://www.youtube.com/watch?v=px_4TxC2mXU): \n",
    "> I learned very early the difference between knowing the name of something and knowing something.\" - Richard Feynman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bottom-up explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Curse of Knowledge](https://en.wikipedia.org/wiki/Curse_of_knowledge) leads to *top-down* explanations:\n",
    "\n",
    "![](img/top_down.png)\n",
    "\n",
    "- When you know something well, you think about things in the context of all your knowledge. \n",
    "- Those lacking the context, or frame of mind, cannot easily understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is another way: *bottom-up* explanations:\n",
    "\n",
    "![](img/bottom_up.png)\n",
    "\n",
    "When you're brand new to a concept, you benefit from analogies, concrete examples and familiar patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples, which one represented a bottom-up explanation and which one a top-down explanation?\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New ideas in small chunks\n",
    "\n",
    "The first explanation has a hidden conceptual skeleton:\n",
    "\n",
    "1. The concept of setting a bunch of values.\n",
    "2. Random forest example.\n",
    "3. The problem / pain point.\n",
    "4. The solution.\n",
    "5. How it works - high level.\n",
    "6. How it works - written example.\n",
    "7. How it works - code example.\n",
    "8. The name of what we were discussing all this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reuse your running examples\n",
    "\n",
    "Effective explanations often use the same example throughout the text and code. This helps readers follow the line of reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach from all angles\n",
    "\n",
    "When we're trying to draw mental boundaries around a concept, it's helpful to see examples on all sides of those boundaries. If we were writing a longer explanation, it might have been better to show more, e.g.\n",
    "\n",
    "- Performance with and without hyperparameter tuning. \n",
    "- Other types of hyperparameter tuning (e.g. `RandomizedSearchCV`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When experimenting, show the results asap\n",
    "\n",
    "The first explanation shows the output of the code, whereas the second does not. This is easy to do and makes a big difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interesting to you != useful to the reader (aka it's not about you)\n",
    "\n",
    "Here is something which was deleted from the explanation:\n",
    "\n",
    "> Some hyperparameters, like `n_estimators` are numeric. Numeric hyperparameters are like the knobs in the cockpit: you can tune them continuously. `n_estimators` is numeric. Categorical hyperparameters are like the switches in the cockpit: they can take on (two or more) distinct values. `criterion` is categorical. \n",
    "\n",
    "It's a very elegant analogy! But is it helpful?\n",
    "\n",
    "And furthermore, what is my hidden motivation for wanting to include it? Elegance, art, and the pursuit of higher beauty? Or _making myself look smart_? So maybe another name for this principle could be **It's not about you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML and decision-making (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "\n",
    "Imagine you are tasked with developing a recommender system for YouTube. You have data on which users clicked on which videos. After spending considerable time building a recommender system using this data, you realize it isn't producing high-quality recommendations. What could be the reasons for this?\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think beyond the data that's given to you. Questions you have to consider:\n",
    "\n",
    "- Who is the decision maker?\n",
    "- What are their objectives?\n",
    "- What are their alternatives?\n",
    "- What is their context?\n",
    "- What data do I need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is often a wide gap between what people care about and what ML can do.\n",
    "- To understand what ML can do, let's think about what **decisions** will be made using ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decisions involve a few key pieces\n",
    "\n",
    "- The **decision variable**: the variable that is manipulated through the decision.\n",
    "  - E.g. how much should I sell my house for? (numeric)\n",
    "  - E.g. should I sell my house? (categorical)\n",
    "- The decision-maker's **objectives**: the variables that the decision-maker ultimately cares about, and wishes to manipulate indirectly through the decision variable.\n",
    "  - E.g. my total profit, time to sale, etc.\n",
    "- The **context**: the variables that mediate the relationship between the decision variable and the objectives.\n",
    "  - E.g. the housing market, cost of marketing it, my timeline, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence and `predict_proba` (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What does it mean to be \"confident\" in your results?\n",
    "- When you perform analysis, you are responsible for many judgment calls.\n",
    "- [Your results will be different than others](https://fivethirtyeight.com/features/science-isnt-broken/#part1).\n",
    "- As you make these judgments and start to form conclusions, how can you recognize your own uncertainties about the data so that you can communicate confidently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does this mean for us, when we're trying to make claims about our data?\n",
    "\n",
    "Let's imagine that the following claim is true:\n",
    "\n",
    "> Vancouver has the highest cost of living of all cities in Canada.\n",
    "\n",
    "Now let's consider a few beliefs we could hold:\n",
    "\n",
    "1. Vancouver has the highest cost of living of all cities in Canada. **I am 95% sure of this.** \n",
    "2. Vancouver has the highest cost of living of all cities in Canada. **I am 55% sure of this.** \n",
    "\n",
    "The part is bold is called a [credence](https://en.wikipedia.org/wiki/Credence_(statistics)). Which belief is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if it's actually Toronto that has the highest cost of living in Canada?\n",
    "\n",
    "1. Vancouver has the highest cost of living of all cities in Canada. **I am 95% sure of this.** \n",
    "2. Vancouver has the highest cost of living of all cities in Canada. **I am 55% sure of this.** \n",
    "\n",
    "Which belief is better now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Conclusion: We don't just want to be right. We want to be confident when we're right and hesitant when we're wrong.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "\n",
    "In our final exam, imagine if, along with your answers, we ask you to also provide a confidence score for each. This would involve rating how sure you are about each answer, perhaps on a percentage scale from 0% (completely unsure) to 100% (completely sure). This method not only assesses your knowledge but also your awareness of your own understanding, potentially impacting the grading process and highlighting areas for improvement. Who supports this idea 😉? \n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does this have to do with applied ML?\n",
    "\n",
    "- What if you `predict` that a credit card transaction is fraudulent?\n",
    "  - We probably want `predict_proba` a lot of the time.\n",
    "- What if `predict_proba` is 0.95 in that case?\n",
    "  - How confident are YOU?\n",
    "- What if you forecast that avocado prices will go up next week? \n",
    "  - How confident are you there?\n",
    "- Or what if you predict a house price to be \\\\$800k? \n",
    "  - That is not even a true/false statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preferences in `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call `fit` for `LogisticRegression` it has similar preferences: \n",
    "<br>\n",
    "<span style=\"color:green\">**correct and confident**</span> <br> **>** <span style=\"color:cyan\"> **correct and hesitant** </span>  <br> **>** <span style=\"color:orange\">**incorrect and hesitant**</span> <br> **>**  <span style=\"color:red\">**incorrect and confident**</span> \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a \"loss\" or \"error\" function like mean squared error, so lower values are better.\n",
    "- When you call `fit` it tries to minimize this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should be the loss? (Activity: 4 mins)\n",
    "\n",
    "- Consider the following made-up classification example where target (true `y`) is binary: -1 or 1. \n",
    "- The true $y$ (`y_true`) and models raw scores ($w^Tx_i$) are given to you. \n",
    "- You want to figure out how do you want to punish the mistakes made by the current model.\n",
    "- How will you punish the model in each case?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"y_true\": [1, 1, 1, 1, -1, -1, -1, -1],\n",
    "    \"raw score ($w^Tx_i$)\": [10.0, 0.51, -0.1, -10, -12.0, -1.0, 0.4, 18.0],\n",
    "    \"correct? (yes/no)\":[\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\"],\n",
    "    \"confident/hesitant?\":[\"confident\", \"hesitant\", \"hesitant\", \"confident\", \"confident\", \"hesistant\", \"hesitant\", \"confident\"],\n",
    "    \"punishment\":[\"None\", \"small punishment\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(-2, 2, 1000)\n",
    "plot_loss_diagram()\n",
    "plt.plot(grid, np.log(1 + np.exp(-grid)), color=\"green\", linewidth=2, label=\"logistic\")\n",
    "plt.legend(loc=\"best\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you are confident and correct, the loss is much smaller \n",
    "- When you are hesitant and correct, the loss is a bit higher\n",
    "- If you are hesitant and incorrect the loss is even higher\n",
    "- If you are confident and incorrect the loss is quite high. \n",
    "- Your loss goes to 0 as you approach 100% confidence in the correct answer.\n",
    "- Your loss goes to infinity as you approach 100% confidence in the incorrect answer.\n",
    "- The above picture shows loss for one example. \n",
    "- The real `LogisticRegression` takes summations of losses over all examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Some nice examples: \n",
    "- (Optional) See also the very related [How to assign partial credit on an exam of true-false questions?](https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/)\n",
    "- [Scott Alexander](https://slatestarcodex.com/2019/01/22/2018-predictions-calibration-results/)\n",
    "  - Look at how the plot starts at 50%. That is because being 40% confident of \"X\" is the same as being 60% confident of \"not X\".\n",
    "- [Good Judgment Project](https://www.gjopen.com/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Very powerful but at the same time can be misleading if not done properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pre-viewing review from [Calling BS visualization videos](https://www.youtube.com/watch?v=T-5aLbNeGo0&list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&index=30&t=0s):\n",
    "\n",
    "- Dataviz in the popular media.\n",
    "  - e.g. [modern NYT](https://youtu.be/T-5aLbNeGo0?t=367)\n",
    "- Misleading axes.\n",
    "  - e.g. [vaccines](https://youtu.be/9pNWVMxaFuM?t=299)\n",
    "- Manipulating bin sizes.\n",
    "  - e.g. [tax dollars](https://youtu.be/zAg1wsYfwsM?t=196)\n",
    "- Dataviz ducks.\n",
    "  - e.g. [drinking water](https://youtu.be/rmii1hfP6d4?t=169)\n",
    "  - \"look how clever we are about design\" -> making it about me instead of about you (see last class)\n",
    "- Glass slippers.\n",
    "  - e.g. [internet marketing tree](https://youtu.be/59teS0SUHtI?t=285)\n",
    "- The principle of proportional ink.\n",
    "  - e.g. [most read books](https://youtu.be/oNhusd3xFC4?t=147)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [Demo of cleaning up a plot](https://www.darkhorseanalytics.com/blog/data-looks-better-naked)\n",
    "- [Principle of proportional ink](https://serialmentor.com/dataviz/proportional-ink.html) from a viz textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/), which we used in lecture 10. As usual, to run this notebook you'll need to download the data. For this dataset, train and test have already been separated. We'll be working with the train portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's separate `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"SalePrice\"])\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SalePrice\"])\n",
    "y_test = test_df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature types \n",
    "\n",
    "- We have mixed feature types and a bunch of missing values. \n",
    "- Now, let's identify feature types and transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "drop_features = [\"Id\"]\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    [\"NA\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The remaining features are categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)\n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying feature transformations\n",
    "\n",
    "- Since we have mixed feature types, let's use `ColumnTransformer` to apply different transformations on different features types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "# Calling fit to examine all the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "new_columns = (\n",
    "    numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), index=X_train.index, columns=new_columns\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_test), index=X_test.index, columns=new_columns\n",
    ")\n",
    "X_test_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training random forests and gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare sklearn's `GradientBoostingRegressor` to `RandomForestRegressor` for different values of `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_values = [3, 10, 30, 100, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "score_rf_train = list()\n",
    "score_rf_test = list()\n",
    "score_gb_train = list()\n",
    "score_gb_test = list()\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    print(n_estimators)\n",
    "    rf = TransformedTargetRegressor(\n",
    "        RandomForestRegressor(n_estimators=n_estimators, random_state=123),\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1,\n",
    "    )\n",
    "    rf.fit(X_train_enc, y_train)\n",
    "    score_rf_train.append(rf.score(X_train_enc, y_train))\n",
    "    score_rf_test.append(rf.score(X_test_enc, y_test))\n",
    "\n",
    "    gb = TransformedTargetRegressor(\n",
    "        GradientBoostingRegressor(n_estimators=n_estimators, random_state=123),\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1,\n",
    "    )\n",
    "    gb.fit(X_train_enc, y_train)\n",
    "    score_gb_train.append(gb.score(X_train_enc, y_train))\n",
    "    score_gb_test.append(gb.score(X_test_enc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusing and perhaps misleading visualization of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a low-quality plot that is confusing and perhaps downright misleading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6));\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(n_estimators_values, score_rf_train, label=\"rf train\")\n",
    "plt.plot(n_estimators_values, score_rf_test, label=\"rf test\")\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(n_estimators_values, score_gb_train, label=\"gb train\")\n",
    "plt.plot(n_estimators_values, score_gb_test, label=\"gb test\")\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Could you create some visualizations such that it makes\n",
    "\n",
    "- RF look better than GB\n",
    "- GB look better than RF\n",
    "- RF and GB look equally good\n",
    "\n",
    "You can create your own misleading example and copy or screenshot it and paste it into [this Google document](https://docs.google.com/document/d/1Nqpcx4yHuVhyrDFrpxbBnduoJC4VFnvVybqzIA1aUmM/edit?usp=sharing).\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some misleading plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RF better than GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "nmax = 3\n",
    "\n",
    "# plt.plot(n_estimators_values[:nmax], score_rf_train[:nmax], label=\"rf train\")\n",
    "plt.plot(n_estimators_values[:nmax], score_rf_test[:nmax], label=\"rf test\")\n",
    "# plt.plot(n_estimators_values[:nmax], score_gb_train[:nmax], label=\"gb train\")\n",
    "plt.plot(n_estimators_values[:nmax], score_gb_test[:nmax], label=\"gb test\")\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.legend()\n",
    "plt.title(\"for most values of n_estimators, RF is betteR!!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GB better than RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_estimators_values, score_rf_test, label=\"rf test\")\n",
    "plt.plot(n_estimators_values, score_gb_test, label=\"gb test\")\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.legend()\n",
    "plt.title(\"GB better!!!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Equally good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 4\n",
    "\n",
    "# plt.plot(n_estimators_values, score_rf_train, label=\"rf train\")\n",
    "plt.plot(n_estimators_values[:nmax], score_rf_test[:nmax], \"b\", label=\"rf test\")\n",
    "plt.ylabel(\"RF $R^2$ score\")\n",
    "plt.ylim((-10, 1.2))\n",
    "plt.legend(loc=0)\n",
    "plt.twinx()\n",
    "# plt.plot(n_estimators_values, score_gb_train, label=\"gb train\")\n",
    "plt.plot(n_estimators_values[:nmax], score_gb_test[:nmax], \"--r\", label=\"gb test\")\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"n estimators\")\n",
    "plt.ylabel(\"GB $R^2$ score\")\n",
    "plt.ylim((-10, 1.25))\n",
    "#plt.ylim((-0.01, 0.70))\n",
    "plt.title(\"Both equally good!!!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to watch out for\n",
    "\n",
    "- Chopping off the x-axis\n",
    "    - the practice of starting the x-axis (or sometimes the y-axis) at a value other than zero to exaggerate the changes in the data   \n",
    "- Saturate the axes\n",
    "    - where the axes are set to ranges that are too narrow or too wide for the data being presented making it difficult to identify patterns\n",
    "- Bar chart for a cherry-picked values\n",
    "- Different y-axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Be critical of your visualizations and try to make them as honest as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principles of effective communication\n",
    "  - Concepts then labels, not the other way around\n",
    "  - Bottom-up explanations\n",
    "  - New ideas in small chunks\n",
    "  - Reuse your running examples\n",
    "  - Approaches from all angles  \n",
    "  - When experimenting, show the results asap\n",
    "  - **It's not about you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Decision variables, objectives, and context.\n",
    "- How does ML fit in?\n",
    "- Expressing your confidence about the results\n",
    "- Misleading visualizations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Have a great weekend! \n",
    "\n",
    "![](img/eva-seeyou.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "cpsc330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
